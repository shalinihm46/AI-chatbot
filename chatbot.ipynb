{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "170bfab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304a1de2",
   "metadata": {},
   "source": [
    "# importing and reading corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1026e050",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     self signed certificate in certificate chain\n",
      "[nltk_data]     (_ssl.c:1129)>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     self signed certificate in certificate chain\n",
      "[nltk_data]     (_ssl.c:1129)>\n"
     ]
    }
   ],
   "source": [
    "f=open('R20EF379_data.csv','r',errors='ignore')\n",
    "raw_doc=f.read()\n",
    "raw_doc=raw_doc.lower()#convert txt to lowercase\n",
    "nltk.download('punkt')#using punk tokenizer\n",
    "nltk.download('wordnet')#lexical dataset\n",
    "sent_tokens=nltk.sent_tokenize(raw_doc)\n",
    "word_tokens=nltk.word_tokenize(raw_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "532e8245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural language processing (nlp) is an interdisciplinary subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.',\n",
       " 'the goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2079ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural', 'language']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cc23d7",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "969433ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer=nltk.stem.WordNetLemmatizer()\n",
    "#wordnet is a semantically-oriented dictionary of english\n",
    "def LemTokens(tokens):\n",
    "  return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict=dict((ord(punct),None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "  return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ea0f4",
   "metadata": {},
   "source": [
    "# definig the greeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fb032a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "GREET_INPUTS=(\"hello\",\"hi\",\"greeting\",\"sup\",\"what's up\",\"hey\")\n",
    "GREET_RESPONSES=[\"hi\",\"hey\",\"*nods*\",\"hi there\",\"hello\",\"i am glad! yoy ara talking to me\"]\n",
    "def greet(sentence):\n",
    "  for word in sentence.split():\n",
    "    if word.lower() in GREET_INPUTS:\n",
    "      return random.choice(GREET_RESPONSES)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38c0880",
   "metadata": {},
   "source": [
    "# Response generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8513b682",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer#term frequency and inverse document freq(how rare the word is occured)\n",
    "from sklearn.metrics.pairwise import cosine_similarity#how similar the documents are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "667e7eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "  robo1_response=''\n",
    "  TfidfVec=TfidfVectorizer(tokenizer=LemNormalize,stop_words='english')\n",
    "  tfidf=TfidfVec.fit_transform(sent_tokens)\n",
    "  vals=cosine_similarity(tfidf[-1],tfidf)\n",
    "  idx=vals.argsort()[0][-2]\n",
    "  flat=vals.flatten()\n",
    "  flat.sort()\n",
    "  req_tfidf=flat[-2]\n",
    "  if(req_tfidf==0):\n",
    "    robo1_response=robo1_response+\"i am sorry! i dont understand you\"\n",
    "    return robo1_response\n",
    "  else:\n",
    "    robo1_response=robo1_response+sent_tokens[idx]\n",
    "    return robo1_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ca3a79",
   "metadata": {},
   "source": [
    "# Defining conversation start/end protocols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7113ce2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading omw-1.4: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     self signed certificate in certificate chain\n",
      "[nltk_data]     (_ssl.c:1129)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83428aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag=True\n",
    "print(\"BOT:My name is stark.Let's have a conversation also,if you want to exit any time ,just type Bye!\")\n",
    "while(flag==True):\n",
    "  user_response=input()\n",
    "  user_response=user_response.lower()\n",
    "  if(user_response!='bye'):\n",
    "    if(user_response=='thanks' or user_response=='thank you'):\n",
    "      flag=False\n",
    "      print(\"BOT: you are welcome..\")\n",
    "    else:\n",
    "      if(greet(user_response)!=None):\n",
    "        print(\"BOT:\"+greet(user_response))\n",
    "      else:\n",
    "        sent_tokens.append(user_response)\n",
    "        word_tokens=word_tokens+nltk.word_tokenize(user_response)\n",
    "        final_words=list(set(word_tokens))\n",
    "        print(\"BOT: \",end=\"\")\n",
    "        print(response(user_response))\n",
    "        sent_tokens.remove(user_response)\n",
    "\n",
    "  else:\n",
    "    flag=False\n",
    "    print(\"BOT: Goodbye! Take care <3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10fd246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aad6d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfd712b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
